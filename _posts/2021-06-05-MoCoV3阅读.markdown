---
layout: post
title: 《An Empirical Study of Training Self-Supervised Vision Transformers》论文阅读
date: 20210-06-05 19:20:23 +0900
category: DL
---
# 《An Empirical Study of Training Self-Supervised Vision Transformers》论文阅读
## 简介

这篇论文没有什么新的方法。这一年Transformer在视觉领域大展拳脚，许多基础的视觉应用都开始尝试使用Transformer代替CNN，其中比较著名的是**Vision Transformers（ViT）**。这篇论文之之前MoCo的基础上进行改进，并且将ViT作为backbone来进行研究（分别从batch size大小，学习率、优化器来研究不同的模块对MoCo v3这种自监督学习框架带来的影响。同时作者通过实验发现ViT在训练的时候会出现不稳定的情况，并且这种不稳定不易被发现，因此提出了一种改进措施。

## MoCo v3

相比于MoCo v1/2，MoCo v3最大的改进是去掉了memory queue。因为作者发现在使用足够大的batch size的时候memory queue带来的效果不佳。

![image-20210705214712810](../assets/img/MoCoV3/image-20210705214712810.png)

## Self-Supervised ViT训练不稳定性研究

### Batch size大小

![image-20210605155830029](../assets\img\MoCoV3\Batch_size.png)

作者首先从batch size的大小出发，发现随着batch size的增大，效果越来越好（负样本量增加）。但是随着batch size增大到6K左右时效果反而变差了。作者通过对实验重启，使得模型跳出当前局部最优的点发现模型的精度取决于重启的好坏，并且当这种变坏的效果出现时也仅仅只是有轻微的降低准确率，这种变坏往往会被忽略。

### 学习率

![image-20210605161306872](../assets\img\MoCoV3\lr.png)

当学习率较小时往往会更加稳定，但是过小会出现欠拟合的情况。当学习率过大时，就会导致训练不稳定。

### 优化器

![image-20210605162138372](../assets\img\MoCoV3\optimizer.png)

作者使用LAMB优化器在lr=5e-4时能够取得一个较好的结果，但是随着学习率的增大，模型在训练集上的精度开始出现显著降低。作者认为是LAMB能够避免突然性的梯度的变化，但是这种负面的影响会随着累加突显出来。

### 原因

![image-20210705221627143](../assets/img/MoCoV3/image-20210705221627143.png)

## 改善训练稳定性的技巧

作者通过实验发现最早出现这种不稳定的情况往往是前面几层，随着迭代次数的增多，这种不稳定开始传播到最后一层，因此作者就想如果把前面的patch projection固定住，那这样是不是可以有效减轻这种训练的不稳定性。

![image-20210605164938362](../assets/img/MoCoV3/%E5%9B%BA%E5%AE%9Apath%E5%B1%82.png)

从上面的图中可以看到，固定了前面的patch层之后，精度上确实比不固定要好很多，并且训练曲线更加平滑。

另外，作者还在一些常用的自监督的学习框架中使用ViT进行训练，发现通过冻结前面的patch 映射层对这些自监督学习的框架照样有用，特别是对于SwAV，当发生训练不稳定的情况下，loss会变成NaN，但是通过冻结前面的path层之后，能够有效的改善这种训练不稳定的情况，并且还提升了精度。作者还通过与BatchNorm、WeightNorm、gradient clip这些训练技巧进行对比，通过实验发现，使用BatchNorm、WeightNorm在可学习的patch层上的效果没有固定住patch层的效果显著，并且gradient clip的阈值设定的足够小的时候也能够有效改善这种不稳定性（相当于冻结住网络）。

![image-20210605170518364](../assets/img/MoCoV3/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94.png)

## 论文作者的一些看法

通过实验证明ViT前面的patch projection层并不是必须的，最初在ViT中使用patch projection只是为了使得输入能够满足Transformer的形式，因此这种patch projection其实是过于完备的，并且通过上面的实验可以发现，随机的patch 映射是能够足以保存有效的信息的，并且使得数据满足Transformer。这表明造成这种不稳定的根本原因在于优化，这种冻结patch projection的方法能够有效缓解这种不稳定性，但是并没有从根本上解决这种问题，当lr过大时，第一层不太可能是不稳定的根本原因，而是所有的层，只不过往往第一层是非Transformer层，所以更容易处理而已。

## 实验细节

- 长warm up有助于减轻模型训练的不稳定性（40个epoch）