<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>DreamMr Page</title>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  
  <!-- Font Awesome -->
  <link rel="stylesheet" type="text/css" href="/assets/css/fontawesome-all.min.css">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon.ico">

  <!-- Google Analytics -->
  

</head>


  <body>
    <nav class="nav">
      <div class="nav-container">
        <a href="/">
          <h2 class="nav-title">DreamMr Page</h2>
        </a>
        <ul>
          <li><a href="/">About</a></li>
          <li><a href="/portfolio/">Publications</a></li>
        </ul>
    </div>
  </nav>

    <main>
      <div class="post">
  <h2 class="post-title">论文《Improving Contrastive Learning by Visualizing feature Transformation》笔记</h2>
  <div class="post-line"></div>

  <p>该论文为iccv2021自监督领域oral中的一篇。</p>

<h1 id="improving-contrastive-learning-by-visualizing-feature-transformation">Improving Contrastive Learning by Visualizing feature Transformation</h1>

<h2 id="动机">动机</h2>

<p>以往自监督的对比学习方法，大多通过数据增强的方式来获得多种视图，但是数据增强的方式需要进行探索，并且不能直观上来保证一定有效。那么这篇论文首先通过实验证明：</p>

<ul>
  <li>负样本不应该有非常剧烈且明显的变化，对于训练过程中，负样本的变化应该是稳定且平滑的变化。</li>
  <li>对于正样本对，他们之间的分数应该越小（代表困难正样本对）能够有效改善关于样本的表示。</li>
</ul>

<p>作者通过一系列的实验图表证明上述两点对自监督学习的作用，并通过Feature Transformation的方法在特征层面进行增强，并且这种方式的可解释性比直接在图像上做数据增强可解释性较高。</p>

<h2 id="可视化对比学习">可视化对比学习</h2>

<p>作者在MoCo上做的实验以及相关的可视化。</p>

<h3 id="moco回顾">MoCo回顾</h3>

<p><img src="../assets/img/FT/moco.png" alt="image-20211109210315869" />
\(\theta_{f_k}\leftarrow m\theta_{f_k}+(1-m)\theta_{f_q}\)</p>

<h3 id="发生模型坍塌的分析">发生模型坍塌的分析</h3>

<p>作者通过实验发现，使m变小会带来更多的不一致性，即负样本变化太大，导致训练过程中梯度的变化也较大，这样就使模型很难收敛。</p>

<p><img src="../assets/img/FT/pos_neg_score_statistics.png" alt="image-20211109211508016" /></p>

<p><img src="../assets/img/FT/gradient.png" alt="image-20211109211548784" /></p>

<p>因此作者通过分析得出，负样本在训练过程中不能发生过于剧烈的变化，应该稳定平稳的进行变化。</p>

<h3 id="困难正样本对能够有效改善模型的训练">困难正样本对能够有效改善模型的训练</h3>

<p>把m变小，同时还能使得$f_q$和$f_k$变得更为相似，这样就会导致正样本对之间的距离变得很小，从而影响了模型能够学到更鲁棒的表达。</p>

<p><img src="../assets/img/FT/pos_neg_score.png" alt="image-20211109214006204" /></p>

<p>因此作者认为，通过增大正样本对之间的距离，使其变成困难样本能够让模型学到更鲁棒的表达。</p>

<h2 id="feature-transformation">Feature Transformation</h2>

<h3 id="正样本外插">正样本外插</h3>

<p><img src="../assets/img/FT/positive_extrapolation.png" alt="image-20211109214313050" /></p>

<p>作者在论文中通过外插的方法，增大正样本对之间的距离。
\(\hat{z}_q=\lambda_{ex}z_q+(1-\lambda_{ex})z_{k+}\\
\hat{z}_{k+}=\lambda_{z_{k+}}+(1-\lambda_{ex})z_q\)
为了使得插值之后的的正样本对之间的距离进一步变远，应该使得$\lambda_{ex}\ge1$。作者在后面的实验过程中，还引入了一种非标量的$\lambda_{ex}$，同时还引入了向量级别的，也就是针对向量中的每一维都有一个对应的$\lambda_{ex}$。$\lambda_{ex}\sim Beta(\alpha_{ex},\alpha_{ex})$</p>

<h3 id="负样本对插值">负样本对插值</h3>

<p>为了增加负样本对的多样性，通过将queue中的负样本进行线性插值的方法得到新的负样本：
\(\hat{Z}_{neg}=\lambda_{in}\cdot Z_{neg}+(1-\lambda_{in})\cdot Z_{perm}\)
其中$Z_{neg}$为queue中所有负样本，$Z_{perm}$为queue中负样本进行随机打乱后重组的负样本。$\lambda_{in}\sim Beta(\alpha_{in},\alpha_{in})$</p>

<p><img src="../assets/img/FT/negative_interpolation.png" alt="" /></p>

<h2 id="实验">实验</h2>

<p>作者在实验中说明，当一开始就使用Feature Transformation会使得模型变得混乱，因此最好是在第二个epoch之后开始使用会更好。</p>


</div>

<div class="pagination">
  
    <a href="/2022-04-19/Docker%E5%AF%BC%E5%87%BA%E9%95%9C%E5%83%8F%E5%92%8C%E5%AF%BC%E5%85%A5%E9%95%9C%E5%83%8F" class="left next">Prev</a>
  
  
    <a href="/2021-09-12/Docker%E6%90%AD%E5%BB%BAdeeplearning%E7%8E%AF%E5%A2%83" class="right next">Next</a>
  

  <a href="#" class="top">Top</a>
</div>

    </main>


    <footer>
      <span>
        &copy; <time datetime="2025-05-02 16:58:42 +0800">2025</time> Wenbin Wang. <a href="https://github.com/kssim/about-portfolio/">A.P</a> theme by kssim.
      </span>
    </footer>
  </body>
</html>
